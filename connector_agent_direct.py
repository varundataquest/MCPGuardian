#!/usr/bin/env python3
"""
MCP Guardian Connector Agent
Generates runnable code for connecting to MCP servers
"""

import asyncio
import json
import os
from typing import Dict, Any, List
from pathlib import Path

class ConnectorAgent:
    def __init__(self):
        self.name = "MCP Guardian Connector Agent"
    
    async def run(self, prompt: str, server_info: Dict[str, Any], framework: str = "langchain") -> Dict[str, Any]:
        """Generate connection code for a specific server and framework"""
        
        server_name = server_info['name']
        endpoint = server_info['endpoint']
        auth_model = server_info.get('auth_model', 'api_key')
        capabilities = server_info.get('capabilities', [])
        
        # Generate framework-specific code
        if framework == "langchain":
            code = self._generate_langchain_code(server_name, endpoint, auth_model, capabilities, prompt)
        elif framework == "autogen":
            code = self._generate_autogen_code(server_name, endpoint, auth_model, capabilities, prompt)
        elif framework == "langgraph":
            code = self._generate_langgraph_code(server_name, endpoint, auth_model, capabilities, prompt)
        else:
            code = self._generate_custom_code(server_name, endpoint, auth_model, capabilities, prompt)
        
        instructions = self._generate_setup_instructions(server_name, auth_model)
        files = self._generate_additional_files(server_name, auth_model)
        
        return {
            "framework": framework,
            "code": code,
            "instructions": instructions,
            "files": files
        }
    
    def _generate_langchain_code(self, server_name: str, endpoint: str, auth_model: str, capabilities: List[str], prompt: str) -> str:
        """Generate LangChain code for connecting to MCP server"""
        
        # Determine the main capability based on the prompt
        main_capability = self._extract_main_capability(prompt, capabilities)
        
        code = f'''#!/usr/bin/env python3
"""
MCP Agent for {server_name}
Generated by MCP Guardian
Task: {prompt}
"""

import os
import asyncio
from typing import List, Dict, Any
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from modelcontextprotocol import ClientSession, StdioServerParameters
from modelcontextprotocol.client import ClientSession
from modelcontextprotocol.models import TextContent

class {self._to_class_name(server_name)}Tool(BaseTool):
    """Tool for interacting with {server_name}"""
    
    name = "{server_name.replace('-', '_')}"
    description = "Interact with {server_name} for {main_capability}"
    
    def __init__(self, session: ClientSession):
        super().__init__()
        self.session = session
    
    async def _arun(self, query: str) -> str:
        """Execute the tool asynchronously"""
        try:
            # Send request to MCP server
            response = await self.session.call_tool(
                name=self.name,
                arguments={{"query": query}}
            )
            return response.content[0].text
        except Exception as e:
            return f"Error: {{str(e)}}"

class {self._to_class_name(server_name)}Agent:
    """Agent for {server_name} operations"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.session = None
        self.agent_executor = None
    
    async def connect_to_server(self):
        """Connect to the MCP server"""
        try:
            # Connect to MCP server
            server_params = StdioServerParameters(
                command="{endpoint}",
                args=[]
            )
            
            self.session = ClientSession(server_params)
            await self.session.initialize()
            
            # Create tool
            tool = {self._to_class_name(server_name)}Tool(self.session)
            
            # Create agent
            prompt = ChatPromptTemplate.from_messages([
                ("system", f"""You are an AI agent that can {main_capability} using {server_name}.
                
Your capabilities include: {', '.join(capabilities)}

Always use the {server_name.replace('-', '_')} tool to perform operations.
Be helpful, efficient, and secure in your responses."""),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{{input}}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])
            
            agent = create_openai_functions_agent(self.llm, [tool], prompt)
            self.agent_executor = AgentExecutor(agent=agent, tools=[tool], verbose=True)
            
            print(f"✅ Connected to {server_name}")
            return True
            
        except Exception as e:
            print(f"❌ Failed to connect to {server_name}: {{str(e)}}")
            return False
    
    async def run(self, user_input: str) -> str:
        """Run the agent with user input"""
        if not self.agent_executor:
            if not await self.connect_to_server():
                return "Failed to connect to server"
        
        try:
            result = await self.agent_executor.ainvoke({{"input": user_input}})
            return result["output"]
        except Exception as e:
            return f"Error: {{str(e)}}"
    
    async def close(self):
        """Close the connection"""
        if self.session:
            await self.session.close()

async def main():
    """Main function to run the agent"""
    print("🚀 Starting {self._to_class_name(server_name)} Agent")
    print(f"📋 Task: {prompt}")
    print(f"🔗 Server: {server_name}")
    print(f"🔐 Auth: {auth_model}")
    print("-" * 50)
    
    # Create agent
    agent = {self._to_class_name(server_name)}Agent()
    
    try:
        # Connect to server
        if not await agent.connect_to_server():
            return
        
        # Interactive loop
        print("\\n💬 Agent is ready! Type 'quit' to exit.")
        while True:
            user_input = input("\\n🤖 You: ")
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            
            print("\\n🔄 Processing...")
            result = await agent.run(user_input)
            print(f"\\n📤 Agent: {{result}}")
    
    except KeyboardInterrupt:
        print("\\n\\n👋 Goodbye!")
    finally:
        await agent.close()

if __name__ == "__main__":
    asyncio.run(main())
'''
        return code
    
    def _generate_setup_instructions(self, server_name: str, auth_model: str) -> str:
        """Generate setup instructions"""
        
        instructions = f"""# Setup Instructions for {server_name}

## 1. Install Dependencies
```bash
pip install langchain langchain-openai modelcontextprotocol
```

## 2. Set Environment Variables
```bash
export OPENAI_API_KEY="your_openai_api_key_here"
"""

        if auth_model == 'oauth2':
            instructions += f"""
export {server_name.upper().replace('-', '_')}_CLIENT_ID="your_client_id"
export {server_name.upper().replace('-', '_')}_CLIENT_SECRET="your_client_secret"
"""
        elif auth_model == 'api_key':
            instructions += f"""
export {server_name.upper().replace('-', '_')}_API_KEY="your_api_key"
"""
        elif auth_model == 'username_password':
            instructions += f"""
export {server_name.upper().replace('-', '_')}_USERNAME="your_username"
export {server_name.upper().replace('-', '_')}_PASSWORD="your_password"
"""

        instructions += f"""
## 3. Run the Agent
```bash
python {server_name.replace('-', '_')}_agent.py
```

## 4. Usage
- The agent will connect to {server_name} automatically
- You can interact with it using natural language
- Type 'quit' to exit

## 5. Security Notes
- Keep your API keys secure
- Never commit credentials to version control
- Use environment variables for sensitive data
"""
        
        return instructions
    
    def _generate_additional_files(self, server_name: str, auth_model: str) -> List[Dict[str, str]]:
        """Generate additional configuration files"""
        
        files = []
        
        # Requirements file
        requirements_content = """langchain>=0.1.0
langchain-openai>=0.1.0
modelcontextprotocol>=0.1.0
python-dotenv>=1.0.0
"""
        files.append({
            "name": "requirements.txt",
            "content": requirements_content,
            "description": "Python dependencies for the agent"
        })
        
        # Environment template
        env_content = f"""# Environment variables for {server_name} agent
OPENAI_API_KEY=your_openai_api_key_here
"""
        
        if auth_model == 'oauth2':
            env_content += f"""
{server_name.upper().replace('-', '_')}_CLIENT_ID=your_client_id
{server_name.upper().replace('-', '_')}_CLIENT_SECRET=your_client_secret
"""
        elif auth_model == 'api_key':
            env_content += f"""
{server_name.upper().replace('-', '_')}_API_KEY=your_api_key
"""
        elif auth_model == 'username_password':
            env_content += f"""
{server_name.upper().replace('-', '_')}_USERNAME=your_username
{server_name.upper().replace('-', '_')}_PASSWORD=your_password
"""
        
        files.append({
            "name": ".env.template",
            "content": env_content,
            "description": "Environment variables template"
        })
        
        # README file
        readme_content = f"""# {server_name} Agent

This agent was generated by MCP Guardian to help you interact with {server_name}.

## Quick Start

1. Copy `.env.template` to `.env` and fill in your credentials
2. Install dependencies: `pip install -r requirements.txt`
3. Run the agent: `python {server_name.replace('-', '_')}_agent.py`

## Features

- Natural language interaction
- Secure credential management
- Error handling and logging
- Easy to extend and customize

## Security

- All credentials are stored in environment variables
- No hardcoded secrets in the code
- Secure connection to MCP server

Generated by MCP Guardian - AI-powered security-first MCP server discovery
"""
        
        files.append({
            "name": "README.md",
            "content": readme_content,
            "description": "Project documentation and setup guide"
        })
        
        return files
    
    def _extract_main_capability(self, prompt: str, capabilities: List[str]) -> str:
        """Extract the main capability from the prompt"""
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ["file", "files", "upload", "download", "storage"]):
            return "file operations"
        elif any(word in prompt_lower for word in ["email", "mail", "send", "receive"]):
            return "email management"
        elif any(word in prompt_lower for word in ["database", "db", "query", "sql"]):
            return "database operations"
        elif any(word in prompt_lower for word in ["search", "find", "index"]):
            return "search operations"
        else:
            return "general operations"
    
    def _to_class_name(self, server_name: str) -> str:
        """Convert server name to valid Python class name"""
        return ''.join(word.capitalize() for word in server_name.replace('-', '_').split('_'))
    
    def _generate_autogen_code(self, server_name: str, endpoint: str, auth_model: str, capabilities: List[str], prompt: str) -> str:
        """Generate AutoGen code for connecting to MCP server"""
        main_capability = self._extract_main_capability(prompt, capabilities)
        
        code = f'''#!/usr/bin/env python3
"""
AutoGen MCP Agent for {server_name}
Generated by MCP Guardian
Task: {prompt}
"""

import os
import asyncio
from typing import List, Dict, Any
from autogen import AssistantAgent, UserProxyAgent, config_list_from_json
from modelcontextprotocol import ClientSession, StdioServerParameters

class {self._to_class_name(server_name)}AutoGenAgent:
    """AutoGen agent for {server_name} operations"""
    
    def __init__(self):
        self.config_list = config_list_from_json(
            env_or_file="OAI_CONFIG_LIST",
            file_location=".",
        )
        self.session = None
    
    async def connect_to_server(self):
        """Connect to the MCP server"""
        try:
            server_params = StdioServerParameters(
                command="{endpoint}",
                args=[]
            )
            
            self.session = ClientSession(server_params)
            await self.session.initialize()
            print(f"✅ Connected to {server_name}")
            
        except Exception as e:
            print(f"❌ Failed to connect to {server_name}: {{str(e)}}")
            raise
    
    async def run(self, user_input: str):
        """Run the AutoGen agent"""
        # Create agents
        assistant = AssistantAgent(
            name="assistant",
            llm_config={{"config_list": self.config_list, "temperature": 0}},
            system_message=f"You are an AI assistant that can {main_capability} using {server_name}. Your capabilities include: {{', '.join(capabilities)}}"
        )
        
        user_proxy = UserProxyAgent(
            name="user_proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=10,
            is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
            code_execution_config={{"work_dir": "workspace"}},
            llm_config={{"config_list": self.config_list, "temperature": 0}}
        )
        
        # Start conversation
        await user_proxy.a_initiate_chat(
            assistant,
            message=user_input
        )

if __name__ == "__main__":
    async def main():
        agent = {self._to_class_name(server_name)}AutoGenAgent()
        await agent.connect_to_server()
        await agent.run("Hello, can you help me with {main_capability}?")
    
    asyncio.run(main())
'''
        return code
    
    def _generate_langgraph_code(self, server_name: str, endpoint: str, auth_model: str, capabilities: List[str], prompt: str) -> str:
        """Generate LangGraph code for connecting to MCP server"""
        main_capability = self._extract_main_capability(prompt, capabilities)
        
        code = f'''#!/usr/bin/env python3
"""
LangGraph MCP Agent for {server_name}
Generated by MCP Guardian
Task: {prompt}
"""

import os
import asyncio
from typing import List, Dict, Any, TypedDict, Annotated
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, END
from modelcontextprotocol import ClientSession, StdioServerParameters

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], "The messages in the conversation"]
    server_response: Annotated[str, "Response from MCP server"]

class {self._to_class_name(server_name)}LangGraphAgent:
    """LangGraph agent for {server_name} operations"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.session = None
        self.workflow = None
    
    async def connect_to_server(self):
        """Connect to the MCP server"""
        try:
            server_params = StdioServerParameters(
                command="{endpoint}",
                args=[]
            )
            
            self.session = ClientSession(server_params)
            await self.session.initialize()
            print(f"✅ Connected to {server_name}")
            
        except Exception as e:
            print(f"❌ Failed to connect to {server_name}: {{str(e)}}")
            raise
    
    def create_workflow(self):
        """Create the LangGraph workflow"""
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("process_input", self._process_input)
        workflow.add_node("call_server", self._call_server)
        workflow.add_node("generate_response", self._generate_response)
        
        # Add edges
        workflow.add_edge("process_input", "call_server")
        workflow.add_edge("call_server", "generate_response")
        workflow.add_edge("generate_response", END)
        
        self.workflow = workflow.compile()
    
    def _process_input(self, state: AgentState) -> AgentState:
        """Process user input"""
        return state
    
    async def _call_server(self, state: AgentState) -> AgentState:
        """Call the MCP server"""
        try:
            # Call server (simplified)
            response = "Server response placeholder"
            state["server_response"] = response
        except Exception as e:
            state["server_response"] = f"Error: {{str(e)}}"
        return state
    
    def _generate_response(self, state: AgentState) -> AgentState:
        """Generate final response"""
        response = self.llm.invoke([
            HumanMessage(content=f"Based on the server response: {{state['server_response']}}, generate a helpful response.")
        ])
        state["messages"].append(response)
        return state
    
    async def run(self, user_input: str):
        """Run the LangGraph workflow"""
        if not self.workflow:
            self.create_workflow()
        
        state = AgentState(
            messages=[HumanMessage(content=user_input)],
            server_response=""
        )
        
        result = await self.workflow.ainvoke(state)
        return result["messages"][-1].content

if __name__ == "__main__":
    async def main():
        agent = {self._to_class_name(server_name)}LangGraphAgent()
        await agent.connect_to_server()
        response = await agent.run("Hello, can you help me with {main_capability}?")
        print(response)
    
    asyncio.run(main())
'''
        return code
    
    def _generate_custom_code(self, server_name: str, endpoint: str, auth_model: str, capabilities: List[str], prompt: str) -> str:
        """Generate custom code for connecting to MCP server"""
        main_capability = self._extract_main_capability(prompt, capabilities)
        
        code = f'''#!/usr/bin/env python3
"""
Custom MCP Agent for {server_name}
Generated by MCP Guardian
Task: {prompt}
"""

import os
import asyncio
import json
from typing import List, Dict, Any
from modelcontextprotocol import ClientSession, StdioServerParameters

class {self._to_class_name(server_name)}CustomAgent:
    """Custom agent for {server_name} operations"""
    
    def __init__(self):
        self.session = None
    
    async def connect_to_server(self):
        """Connect to the MCP server"""
        try:
            server_params = StdioServerParameters(
                command="{endpoint}",
                args=[]
            )
            
            self.session = ClientSession(server_params)
            await self.session.initialize()
            print(f"✅ Connected to {server_name}")
            
        except Exception as e:
            print(f"❌ Failed to connect to {server_name}: {{str(e)}}")
            raise
    
    async def run(self, user_input: str):
        """Run the custom agent"""
        try:
            # Simple interaction with MCP server
            print(f"Processing: {{user_input}}")
            print(f"Capabilities: {{', '.join(capabilities)}}")
            print(f"Main capability: {main_capability}")
            
            # Placeholder for actual server interaction
            response = f"Custom agent response for {{user_input}} using {server_name}"
            return response
            
        except Exception as e:
            return f"Error: {{str(e)}}"

if __name__ == "__main__":
    async def main():
        agent = {self._to_class_name(server_name)}CustomAgent()
        await agent.connect_to_server()
        response = await agent.run("Hello, can you help me with {main_capability}?")
        print(response)
    
    asyncio.run(main())
'''
        return code

# Example usage
if __name__ == "__main__":
    async def test():
        agent = ConnectorAgent()
        
        # Test with a sample server
        server_info = {
            "name": "google-drive-mcp-server",
            "endpoint": "https://api.drive-mcp.com",
            "auth_model": "oauth2",
            "capabilities": ["file_upload", "file_download", "file_sharing"]
        }
        
        result = await agent.run("Agent that can handle file operations", server_info)
        print("Generated Code:")
        print(result["code"])
        print("\nSetup Instructions:")
        print(result["instructions"])
    
    asyncio.run(test()) 